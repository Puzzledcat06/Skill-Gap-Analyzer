ğŸ§  AI Skill Gap Analyzer

An AI-powered Skill Gap Analyzer that evaluates a learnerâ€™s current skills against a target job role and provides:

Strengths and weaknesses

Identified skill gaps

Priority learning areas

Personalized learning roadmap

The system uses interactive, scenario-based questions and dynamically updates the skill map and recommendations based on user responses.

ğŸš€ Live Demo

Deployed App:
ğŸ‘‰ https://skill-gap-analyzer-mine.streamlit.app/

Source Code:
ğŸ‘‰ https://github.com/Puzzledcat06/Skill-Gap-Analyzer

ğŸ¯ Problem Statement

Design and develop an AI-based Skill Gap Analyzer that evaluates a learnerâ€™s current skill level against a target job role and provides:

Strengths and weaknesses

Identified skill gaps

Priority focus areas

Personalized learning recommendations

The system works in an interactive and continuous manner using scenario-based Q&A and skill checkpoints. The evaluation adapts dynamically based on user responses and generates a clear improvement roadmap.

ğŸ§© Features

ğŸ”¹ Multi-role support (ML Engineer, Data Engineer, Frontend Developer, Backend Developer)

ğŸ”¹ Skillset customization per role

ğŸ”¹ Interactive, scenario-based assessment

ğŸ”¹ Dynamic evaluation of core & supporting skills

ğŸ”¹ Skill strengths, needs improvement, and gaps

ğŸ”¹ Priority learning areas & recommended next steps

ğŸ”¹ Free-first AI integration (Groq API)

ğŸ”¹ Deployed on Streamlit Cloud

ğŸ—ï¸ Architecture
skill-gap-analyzer/
â”œâ”€â”€ app.py                 # Streamlit UI
â”œâ”€â”€ ai/                    # AI layer (Groq)
â”‚   â”œâ”€â”€ question_generator.py
â”‚   â””â”€â”€ response_interpreter.py
â”œâ”€â”€ core/                  # Evaluation logic
â”‚   â”œâ”€â”€ schema.py
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â”œâ”€â”€ priority.py
â”‚   â”œâ”€â”€ recommendations.py
â”‚   â””â”€â”€ progress.py (optional)
â”œâ”€â”€ data/                  # Role & skill schemas (JSON)
â”‚   â”œâ”€â”€ ml_engineer.json
â”‚   â”œâ”€â”€ data_engineer.json
â”‚   â”œâ”€â”€ frontend_developer.json
â”‚   â””â”€â”€ backend_developer.json
â””â”€â”€ requirements.txt

ğŸ§  Skill Evaluation Logic

Each role is defined using a structured schema with:

Skills

Subskills

Measurable checkpoints

Importance weights

User responses are classified into:

none, partial, complete

Scores are aggregated from:

Checkpoint â†’ Subskill â†’ Skill

Priority areas are computed by combining:

Role-specific importance weight

Learnerâ€™s proficiency score

This ensures high-impact gaps are prioritized in the roadmap.

ğŸ¤– AI Flow

The AI layer is used to:

Generate scenario-based questions per checkpoint

Interpret free-text responses into structured understanding levels

The final evaluation, prioritization, and recommendations are handled by rule-based logic for explainability and consistency.

This hybrid design ensures robustness while keeping the system adaptive and interactive.

ğŸ§ª Example User Journey

User selects a target role (e.g., Data Engineer)

User customizes the relevant skillset

The system asks scenario-based questions for each skill checkpoint

The AI evaluates responses in real time

The skill map updates dynamically

The final report displays:

Strengths

Needs Improvement

Skill Gaps

High Priority Focus

Recommended Next Steps

The user receives a personalized skill progression roadmap

âš™ï¸ Local Setup (Optional)
git clone https://github.com/Puzzledcat06/Skill-Gap-Analyzer
cd Skill-Gap-Analyzer
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
streamlit run app.py


Set Groq API key (Windows PowerShell):

$env:GROQ_API_KEY="your_api_key_here"

ğŸ“¦ Deployment

The application is deployed on Streamlit Cloud and can be accessed publicly:

ğŸ‘‰ https://skill-gap-analyzer-mine.streamlit.app/

ğŸ“Œ Notes

The AI layer is modular and can be swapped with other providers (e.g., local LLMs via Ollama).

The current implementation demonstrates continuous evaluation and dynamic skill prioritization suitable for prototype and demo purposes.

ğŸ™Œ Acknowledgements

Built as part of an AI skill evaluation challenge to demonstrate adaptive assessment logic, LLM integration, and user-focused learning recommendations.
